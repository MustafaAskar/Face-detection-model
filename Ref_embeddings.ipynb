{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ref embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouWkwFFWB_A8"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import tensorflow as tf\n",
        "#import mtcnn\n",
        "import numpy as np\n",
        "import cv2\n",
        "import copy\n",
        "import PIL\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cropping using Caffe model and OpenCV DNN\n",
        "faces_cropped=[]\n",
        "\n",
        "#load trained model\n",
        "modelFile = \"drive/MyDrive/GP22/res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "configFile = \"drive/MyDrive/GP22/deploy.prototxt.txt\"\n",
        "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
        "\n",
        "#load image\n",
        "img = cv2.imread('drive/MyDrive/GP22/image4.jpg')\n",
        "image_np=np.asarray(img)\n",
        "\n",
        "#keep the original height and width, Caffe model require resizing to 300*300\n",
        "h, w = img.shape[:2]\n",
        "blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0,\n",
        "(300, 300), (104.0, 117.0, 123.0))\n",
        "\n",
        "#path the image to the model\n",
        "net.setInput(blob)\n",
        "\n",
        "#extract\n",
        "faces = net.forward()\n",
        "\n",
        "#crop faces from the image\n",
        "for i in range(faces.shape[2]):\n",
        "        confidence = faces[0, 0, i, 2]\n",
        "        if confidence > 0.5:\n",
        "            box = faces[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (x, y, x1, y1) = box.astype(\"int\")\n",
        "            faces_cropped.append(image_np[y:y1,x:x1])\n"
      ],
      "metadata": {
        "id": "hgzH8wt9CHPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load FaceNet model\n",
        "FaceNet=tf.keras.models.load_model('drive/MyDrive/GP22/facenet_keras.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pac4ZWkcChmw",
        "outputId": "199a2407-1320-4100-b07f-d4d8e270057c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess the image\n",
        "#resize\n",
        "for i in range(len(faces_cropped)):\n",
        "  faces_cropped[i]=PIL.Image.fromarray((faces_cropped[i]*255).astype(np.uint8))\n",
        "  faces_cropped[i]=faces_cropped[i].resize((160,160))\n",
        "  faces_cropped[i]=np.asarray(faces_cropped[i])\n",
        "  \n",
        "  #cast as float\n",
        "  faces_cropped[i]=faces_cropped[i].astype('float32')\n",
        "  \n",
        "  #standraliztion\n",
        "  mean=np.mean(faces_cropped[i])\n",
        "  std=np.std(faces_cropped[i])\n",
        "  faces_cropped[i]=(faces_cropped[i]-mean)/std\n",
        "  \n",
        "  #expand batch dimension so tensorflow can accept it\n",
        "  faces_cropped[i]=np.expand_dims(faces_cropped[i],axis=0)"
      ],
      "metadata": {
        "id": "f2nQ7NjMCk_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict the embeddings\n",
        "embeddings=[]\n",
        "for current_face in faces_cropped:\n",
        "  results=FaceNet.predict(current_face)\n",
        "  embeddings.append(results)\n",
        "\n",
        "#creat dictionary with names\n",
        "ref_embeddings_names={\n",
        "    0:'Mustafa'\n",
        "}"
      ],
      "metadata": {
        "id": "7Q9dNSM0CnB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the embeddings\n",
        "np.save('ref_embeddings',embeddings)\n",
        "\n",
        "with open('ref_embeddings_names.pkl', 'wb') as f:\n",
        "    pickle.dump(ref_embeddings_names, f)"
      ],
      "metadata": {
        "id": "zsD_b9igC1xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FSIuD9ODGeZ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}